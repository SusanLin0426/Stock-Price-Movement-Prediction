{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-01T07:41:35.246199Z",
     "start_time": "2022-04-01T07:41:31.446946Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords_zh.txt', 'r', encoding='utf-8') as file:\n",
    "    stopwords = file.read().splitlines() \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_up = pd.read_csv(\"Up_down_stay/train_tokenStr_list_up.csv\")\n",
    "df_yuanta_up = pd.read_csv(\"Up_down_stay/df_yuanta_up.csv\")\n",
    "df_yuanta_up[\"post_time\"] = pd.to_datetime(df_yuanta_up[\"post_time\"]).dt.date\n",
    "up = pd.concat([df_up, df_yuanta_up[[\"post_time\"]]], axis=1)\n",
    "\n",
    "\n",
    "df_stay = pd.read_csv(\"Up_down_stay/train_tokenStr_list_stay.csv\")\n",
    "df_yuanta_stay = pd.read_csv(\"Up_down_stay/df_yuanta_stay.csv\")\n",
    "df_yuanta_stay[\"post_time\"] = pd.to_datetime(df_yuanta_stay[\"post_time\"]).dt.date\n",
    "stay = pd.concat([df_stay, df_yuanta_stay[[\"post_time\"]]], axis=1)\n",
    "\n",
    "df_down = pd.read_csv(\"Up_down_stay/train_tokenStr_list_down.csv\")\n",
    "df_yuanta_down = pd.read_csv(\"Up_down_stay/df_yuanta_down.csv\")\n",
    "df_yuanta_down[\"post_time\"] = pd.to_datetime(df_yuanta_down[\"post_time\"]).dt.date\n",
    "down = pd.concat([df_down, df_yuanta_down[[\"post_time\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1225, 7142)\n",
      "(504, 7142)\n",
      "(1320, 5400)\n",
      "(684, 5400)\n",
      "(1592, 5714)\n",
      "(351, 5714)\n",
      "(1539, 5248)\n",
      "(279, 5248)\n",
      "(1314, 5402)\n",
      "(409, 5402)\n",
      "(1039, 4430)\n",
      "(425, 4430)\n",
      "(1113, 4646)\n",
      "(373, 4646)\n",
      "(1207, 5010)\n",
      "(236, 5010)\n",
      "(1034, 5116)\n",
      "(402, 5116)\n",
      "(1011, 4894)\n",
      "(447, 4894)\n",
      "(1085, 4686)\n",
      "(265, 4686)\n",
      "(1114, 3612)\n",
      "(371, 3612)\n",
      "(1083, 3488)\n",
      "(401, 3488)\n",
      "(1037, 2832)\n",
      "(314, 2832)\n",
      "(1086, 3982)\n",
      "(387, 3982)\n",
      "(1102, 3680)\n",
      "(231, 3680)\n",
      "(932, 3552)\n",
      "(297, 3552)\n",
      "(915, 3680)\n",
      "(283, 3680)\n",
      "(811, 3714)\n",
      "(333, 3714)\n",
      "(913, 4946)\n",
      "(243, 4946)\n",
      "(859, 4456)\n",
      "(132, 4456)\n"
     ]
    }
   ],
   "source": [
    "# 所有需要當作test的月份\n",
    "total_time = np.arange(np.datetime64(\"2022-06\"), np.datetime64(\"2024-03\"), dtype=\"datetime64[M]\")\n",
    "\n",
    "for month in total_time:\n",
    "    time_test = np.arange(month, month+np.timedelta64(1, \"M\"), dtype='datetime64[D]').tolist()\n",
    "    time_train = np.arange(np.datetime64(time_test[0], \"M\")-np.timedelta64(3, \"M\"), np.datetime64(time_test[0], \"M\"), dtype='datetime64[D]').tolist()\n",
    "\n",
    "    # train\n",
    "    train_startDate = time_train[0]\n",
    "    train_endDate = time_train[-1]\n",
    "    train_tokenStr_list_up = []\n",
    "    train_tokenStr_list_down = []\n",
    "    train_tokenStr_list_stay = []\n",
    "\n",
    "    for t in time_train:\n",
    "        train_tokenStr_list_up += up[up[\"post_time\"] == t][\"0\"].astype(str).tolist()\n",
    "        train_tokenStr_list_down += down[down[\"post_time\"] == t][\"0\"].astype(str).tolist()\n",
    "        train_tokenStr_list_stay += stay[stay[\"post_time\"] == t][\"0\"].astype(str).tolist()\n",
    "\n",
    "    # test\n",
    "    test_startDate = time_test[0]\n",
    "    test_endDate = time_test[-1]\n",
    "    yuanta_up_texts = []\n",
    "    yuanta_down_texts = []\n",
    "    yuanta_stay_texts = []\n",
    "\n",
    "    for t in time_test:\n",
    "        yuanta_up_texts += up[up[\"post_time\"] == t][\"0\"].astype(str).tolist()\n",
    "        yuanta_down_texts += down[down[\"post_time\"] == t][\"0\"].astype(str).tolist()\n",
    "        yuanta_stay_texts += stay[stay[\"post_time\"] == t][\"0\"].astype(str).tolist()\n",
    "\n",
    "    vectorizer_before_chi = TfidfVectorizer(stop_words=stopwords)\n",
    "    X_train_up = vectorizer_before_chi.fit_transform(train_tokenStr_list_up)\n",
    "    X_train_up = pd.DataFrame(X_train_up.toarray(),columns=vectorizer_before_chi.get_feature_names_out())\n",
    "    up_num_rows = X_train_up.shape[1]\n",
    "\n",
    "    X_train_down = vectorizer_before_chi.fit_transform(train_tokenStr_list_down)\n",
    "    X_train_down = pd.DataFrame(X_train_down.toarray(),columns=vectorizer_before_chi.get_feature_names_out())\n",
    "    X_train_down # 56 rows × 1324 columns\n",
    "    down_num_rows = X_train_up.shape[1]\n",
    "\n",
    "    \n",
    "    y_train_up = df_yuanta_up[df_yuanta_up['post_time'].between(train_startDate, train_endDate)]['label']\n",
    "    chi2_selector = SelectKBest(chi2, k = int(min(up_num_rows, down_num_rows)*0.5))\n",
    "    chi2_selector.fit(X_train_up, y_train_up)\n",
    "    kbest_vocabs_up = X_train_up.columns[chi2_selector.get_support()]\n",
    "    X_train_up = X_train_up[kbest_vocabs_up]\n",
    "\n",
    "    y_train_down = df_yuanta_down[df_yuanta_down['post_time'].between(train_startDate, train_endDate)]['label']\n",
    "    chi2_selector = SelectKBest(chi2, k = int(min(up_num_rows, down_num_rows)*0.5))\n",
    "    chi2_selector.fit(X_train_down, y_train_down)\n",
    "    kbest_vocabs_down = X_train_down.columns[chi2_selector.get_support()]\n",
    "    X_train_down = X_train_down[kbest_vocabs_down]\n",
    "\n",
    "    set_up = set(kbest_vocabs_up)\n",
    "    set_down = set(kbest_vocabs_down)\n",
    "\n",
    "    common_words = set_up.intersection(set_down)\n",
    "\n",
    "    unique_up = set_up - common_words\n",
    "    unique_down = set_down - common_words\n",
    "\n",
    "    unique_up_list = list(unique_up) # unique_up_list 代表漲的feature\n",
    "    unique_down_list = list(unique_down) # unique_down_list 代表跌的feature\n",
    "\n",
    "    # print('刪掉同樣的後，漲和跌的個別feature數量：', len(unique_up_list))\n",
    "\n",
    "    total_feature = unique_up_list + unique_down_list\n",
    "    # print('總feature數量：', len(total_feature))\n",
    "\n",
    "    vectorizer = CountVectorizer(lowercase=False, vocabulary=total_feature, binary=True)\n",
    "\n",
    "    yuanta_up_vector = vectorizer.fit_transform(yuanta_up_texts)  # Now pass the list of document strings\n",
    "    yuanta_down_vector = vectorizer.fit_transform(yuanta_down_texts)\n",
    "    yuanta_stay_vector = vectorizer.fit_transform(yuanta_stay_texts)\n",
    "\n",
    "    # create new folder\n",
    "    folder = str(month)\n",
    "    parent_dir = \"./output\"\n",
    "    os.mkdir(os.path.join(parent_dir, folder))\n",
    "\n",
    "    # train vector\n",
    "    yuanta_up_vector = vectorizer.fit_transform(train_tokenStr_list_up) \n",
    "    yuanta_down_vector = vectorizer.fit_transform(train_tokenStr_list_down)\n",
    "    yuanta_stay_vector = vectorizer.fit_transform(train_tokenStr_list_stay)\n",
    "\n",
    "    df_yuanta_up = df_yuanta_up.reset_index(drop=True)\n",
    "    df_yuanta_down = df_yuanta_down.reset_index(drop=True)\n",
    "    df_yuanta_stay = df_yuanta_stay.reset_index(drop=True)\n",
    "\n",
    "    yuanta_up_vector = pd.DataFrame(yuanta_up_vector.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "    yuanta_up_vector['label'] = 1\n",
    "    yuanta_up_vector['post_time'] = df_yuanta_up[df_yuanta_up['post_time'].between(train_startDate, train_endDate)]['post_time'].to_list()\n",
    "\n",
    "    yuanta_down_vector = pd.DataFrame(yuanta_down_vector.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "    yuanta_down_vector['label'] = -1\n",
    "    yuanta_down_vector['post_time'] = df_yuanta_down[df_yuanta_down['post_time'].between(train_startDate, train_endDate)]['post_time'].to_list()\n",
    "\n",
    "    yuanta_stay_vector = pd.DataFrame(yuanta_stay_vector.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "    yuanta_stay_vector['label'] = 0\n",
    "    yuanta_stay_vector['post_time'] = df_yuanta_stay[df_yuanta_stay['post_time'].between(train_startDate, train_endDate)]['post_time'].to_list()\n",
    "\n",
    "    yuanta_vector = pd.concat([yuanta_up_vector, yuanta_down_vector], axis=0)\n",
    "    yuanta_vector = pd.concat([yuanta_vector, yuanta_stay_vector], axis=0)\n",
    "    print(yuanta_vector.shape)\n",
    "\n",
    "    yuanta_vector.to_csv(\"./output/\" + str(month) + \"/train.csv\",index=False)\n",
    "\n",
    "    # test vector\n",
    "    yuanta_up_vector = vectorizer.fit_transform(yuanta_up_texts) \n",
    "    yuanta_down_vector = vectorizer.fit_transform(yuanta_down_texts)\n",
    "    yuanta_stay_vector = vectorizer.fit_transform(yuanta_stay_texts)\n",
    "\n",
    "    yuanta_up_vector = pd.DataFrame(yuanta_up_vector.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "    yuanta_up_vector['label'] = 1\n",
    "    yuanta_up_vector['post_time'] = df_yuanta_up[df_yuanta_up['post_time'].between(test_startDate, test_endDate)]['post_time'].to_list()\n",
    "\n",
    "    yuanta_down_vector = pd.DataFrame(yuanta_down_vector.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "    yuanta_down_vector['label'] = -1\n",
    "    yuanta_down_vector['post_time'] = df_yuanta_down[df_yuanta_down['post_time'].between(test_startDate, test_endDate)]['post_time'].to_list()\n",
    "\n",
    "    yuanta_stay_vector = pd.DataFrame(yuanta_stay_vector.toarray(),columns=vectorizer.get_feature_names_out())\n",
    "    yuanta_stay_vector['label'] = 0\n",
    "    yuanta_stay_vector['post_time'] = df_yuanta_stay[df_yuanta_stay['post_time'].between(test_startDate, test_endDate)]['post_time'].to_list()\n",
    "\n",
    "    yuanta_vector = pd.concat([yuanta_up_vector, yuanta_down_vector], axis=0)\n",
    "    yuanta_vector = pd.concat([yuanta_vector, yuanta_stay_vector], axis=0)\n",
    "    print(yuanta_vector.shape)\n",
    "\n",
    "    yuanta_vector.to_csv(\"./output/\" + str(month) + \"/test.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
